{"cells":[{"cell_type":"markdown","metadata":{"id":"rM5OuN-aGpa5"},"source":["**Module-1: In This Module We Import Dependencies.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11822,"status":"ok","timestamp":1653747482994,"user":{"displayName":"Dhameliya Arpit","userId":"14178346834218186043"},"user_tz":-330},"id":"2ZlDsuE7k0S5","outputId":"f35ec828-f5e0-4880-f43c-905d6735cab0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mtcnn\n","  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n","Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.8.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.21.6)\n","Installing collected packages: mtcnn\n","Successfully installed mtcnn-0.1.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"]}],"source":["!pip install mtcnn\n","!pip install matplotlib\n","from matplotlib import pyplot\n","from matplotlib.patches import Rectangle\n","from matplotlib.patches import Circle\n","from mtcnn.mtcnn import MTCNN\n","import cv2\n","from PIL import Image \n","from numpy import savez_compressed\n","from numpy import asarray\n","from os import listdir\n","from numpy import asarray\n","import csv\n","from time import strftime\n"]},{"cell_type":"markdown","metadata":{"id":"wfTUW7eeGEL-"},"source":["**Module-2: This Module Will Take Image Of Whole Class As Input And Give Single Faces As Output And Store It In Some Dir.(This Modeule Use MTCNN Model\n",")**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"AynemADfwIUC","outputId":"8488dd0d-bbc2-4a49-e004-95fd9a9a7712"},"outputs":[{"name":"stdout","output_type":"stream","text":["saved\n"]},{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d1495eb17619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/FaceNet/combined/2.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# load image from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# create the detector, using default weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2059\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             raise ValueError('Only know how to handle PNG; with Pillow '\n\u001b[1;32m   1463\u001b[0m                              'installed, Matplotlib can handle more images')\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/FaceNet/combined/2.jpg'"]}],"source":["\n","img_crop = []\n","def save_faces(filename, result_list):\n","\t# load the image\n","\tdata = pyplot.imread(filename)\n","\t# plot each face as a subplot\n","\tfor i in range(len(result_list)):\n","\t\t# get coordinates\n","\t\tx1, y1, width, height = result_list[i]['box']\n","\t\tx2, y2 = x1 + width, y1 + height\n","\t\tcv2.imwrite(r\"/content/drive/MyDrive/Facenet/cropped/I_{}.png\".format(i),data[y1:y2, x1:x2])\n","print(\"saved\")\n"," \n","filename = '/content/drive/MyDrive/Facenet/combined/2.jpg'\n","# load image from file\n","pixels = pyplot.imread(filename)\n","# create the detector, using default weights\n","detector = MTCNN()\n","# detect faces in the image\n","faces = detector.detect_faces(pixels)\n","# display faces on the original image\n","save_faces(filename, faces)"]},{"cell_type":"markdown","metadata":{"id":"AYHOg9iYfmgM"},"source":["**Module-3: This Module Is For Removing Face Coordinates from Input Training DataSet.We will load DataSet Of Each Student For Training And Save All Coordinates in compressed file NewDataSet.npz.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"C3HVrhDauQ7M","outputId":"8efc4f0c-2512-4315-d2fc-195a63fe3aca"},"outputs":[{"name":"stdout","output_type":"stream","text":["['user', 'Shyam', 'Sidharth', 'Udit', 'shubham', 'Vimal', 'Tilak', 'Vatsal', 'Smith', 'Tarik', 'Om', 'dhruv', 'Fenil', 'Harshad', 'Harsh', 'HarshBhalala', 'Kashyap', 'Kirtan', 'Manish', 'Shreyash', 'Bhumit', 'Darsh', 'Bhikho', 'Arpit']\n","WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4a29f03b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4a29f03b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1 There are 100 images in the class user:\n","2 There are 100 images in the class Shyam:\n","3 There are 50 images in the class Sidharth:\n","4 There are 100 images in the class Udit:\n","5 There are 100 images in the class shubham:\n","6 There are 100 images in the class Vimal:\n","7 There are 100 images in the class Tilak:\n","8 There are 100 images in the class Vatsal:\n","9 There are 100 images in the class Smith:\n","10 There are 95 images in the class Tarik:\n","11 There are 100 images in the class Om:\n","12 There are 100 images in the class dhruv:\n","13 There are 100 images in the class Fenil:\n","14 There are 100 images in the class Harshad:\n","15 There are 100 images in the class Harsh:\n","16 There are 100 images in the class HarshBhalala:\n","17 There are 100 images in the class Kashyap:\n","18 There are 100 images in the class Kirtan:\n","19 There are 64 images in the class Manish:\n","20 There are 100 images in the class Shreyash:\n","21 There are 100 images in the class Bhumit:\n","22 There are 80 images in the class Darsh:\n","23 There are 100 images in the class Bhikho:\n","24 There are 100 images in the class Arpit:\n","(2289, 160, 160, 3) (2289,)\n"]}],"source":["def extract_image(image):\n","  store_face=[]\n","  img1 = Image.open(image)            \n","  img1 = img1.convert('RGB')          \n","  pixels = asarray(img1)              \n","  detector = MTCNN()                  \n","  f = detector.detect_faces(pixels)\n","  if f != []:\n","   x1,y1,w,h = f[0]['box']             \n","   x1, y1 = abs(x1), abs(y1)\n","   x2 = abs(x1+w)\n","   y2 = abs(y1+h)\n","  #locate the co-ordinates of face in the image\n","   store_face = pixels[y1:y2,x1:x2]\n","  # plt.imshow(store_face)\n","  image1 = Image.fromarray(store_face,'RGB')    #convert the numpy array to object\n","  image1 = image1.resize((160,160))             #resize the image\n","  face_array = asarray(image1)                  #image to array\n","  return face_array\n","\n","def load_faces(directory):\n","  face = []\n","  i=1\n","  for filename in listdir(directory):\n","    path = directory + filename\n","    faces = extract_image(path)\n","    face.append(faces)\n","  return face\n","\n","def load_dataset(directory):\n","  x, y = [],[]\n","  i=1\n","  for subdir in listdir(directory):\n","    path = directory + subdir + '/'\n","    #load all faces in subdirectory\n","    faces = load_faces(path)\n","    #create labels\n","    labels = [subdir for _ in range(len(faces))]\n","    # print(labels)\n","    #summarize\n","    print(\"%d There are %d images in the class %s:\"%(i,len(faces),subdir))\n","    x.extend(faces)\n","    y.extend(labels)\n","    i=i+1\n","  return asarray(x),asarray(y)  \n","\n","print(listdir('/content/drive/MyDrive/Facenet/training/'))\n","#load the datasets\n","trainX,trainY = load_dataset('/content/drive/MyDrive/Facenet/training/')\n","print(trainX.shape,trainY.shape)\n","# print(trainY)\n","#compress the data\n","savez_compressed('/content/drive/MyDrive/Facenet/DataSet.npz',trainX,trainY)"]},{"cell_type":"markdown","metadata":{"id":"farki-VTgWvm"},"source":["**Module-4: In This Module We Will Load Our Pretrained Model Named FaceNet And Then By Loading NewDataSet.npz Obtained From Modeule-3 We Will Extract Embeddings(It is an list of 128 features of face of student) Of Each Training Face And Store It In Compressed File Named NewEmbeddings.npz**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8bXIyVRK1IRz","outputId":"07cfe088-2440-4b27-c325-9194fbbd6cc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2675, 160, 160, 3) (2675,)\n","(2675, 128)\n"]}],"source":["from numpy import load\n","from numpy import asarray\n","from numpy import expand_dims\n","from numpy import savez_compressed\n","from numpy import reshape\n","from keras.models import load_model \n","\n","#Generalize the data and extract the embeddings\n","def extract_embeddings(model,face_pixels):\n","  face_pixels = face_pixels.astype('float32')  #convert the entire data to float32(base)\n","  mean = face_pixels.mean()                    #evaluate the mean of the data\n","  std  = face_pixels.std()                     #evaluate the standard deviation of the data\n","  face_pixels = (face_pixels - mean)/std       \n","  samples = expand_dims(face_pixels,axis=0)    #expand the dimension of data \n","  yhat = model.predict(samples)\n","  return yhat[0]\n","\n","#load the compressed dataset and facenet keras model\n","data = load('/content/drive/MyDrive/FaceNet/DataSet.npz')\n","trainx, trainy = data['arr_0'],data['arr_1']\n","# print(trainy)\n","print(trainx.shape, trainy.shape)\n","model = load_model('/content/drive/MyDrive/FaceNet/facenet_keras.h5',compile=False)\n","\n","#get the face embeddings\n","new_trainx = list()\n","for train_pixels in trainx:\n","  embeddings = extract_embeddings(model,train_pixels)\n","  new_trainx.append(embeddings)\n","new_trainx = asarray(new_trainx)             #convert the embeddings into numpy array\n","print(new_trainx.shape)\n","\n","#compress the 128 embeddings of each face \n","savez_compressed('/content/drive/MyDrive/FaceNet/Embeddings.npz',new_trainx,trainy)"]},{"cell_type":"markdown","metadata":{"id":"RPkRMU_IeaWb"},"source":["**Module-5: This Modeule Will Combine Old Embeddings(Embeddings.npz) With Newly Created Embeddings(NewEmbeddings.npz) Obtained From Module-4 and save it Which Furthure Used At Time Of Testing.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dp6NfHAq0yho"},"outputs":[],"source":["#For Combinning DataSet\n","import numpy as np\n","data_1 = np.load('/content/drive/MyDrive/FaceNet/DataSet.npz')\n","data_2 = np.load('/content/drive/MyDrive/FaceNet/NewDataSet.npz')\n","arr_0 = np.concatenate([data_1['arr_0'], data_2['arr_0']])\n","arr_1 = np.concatenate([data_1['arr_1'], data_2['arr_1']])\n","np.savez('/content/drive/MyDrive/FaceNet/FinalDataSet.npz', arr_0, arr_1)\n","\n","#For Combinning Embeddings\n","data_1 = np.load('/content/drive/MyDrive/FaceNet/embeddings.npz')\n","data_2 = np.load('/content/drive/MyDrive/FaceNet/Newembeddings.npz')\n","arr_0 = np.concatenate([data_1['arr_0'], data_2['arr_0']])\n","arr_1 = np.concatenate([data_1['arr_1'], data_2['arr_1']])\n","np.savez('/content/drive/MyDrive/FaceNet/Finalembeddings.npz', arr_0, arr_1)\n","print(\"saved\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350,"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK","headers":[["content-type","application/javascript"]],"ok":true,"status":200,"status_text":""}}},"id":"_JfyNpZE0OTf","outputId":"42aefcdc-dfc1-4fc7-89b1-c7472ecb4e39"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-f6d21db9-e356-480f-bc87-c0099724de34\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f6d21db9-e356-480f-bc87-c0099724de34\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-21dc3c638f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m   \"\"\"\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    121\u001b[0m   result = _output.eval_js(\n\u001b[1;32m    122\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m--> 123\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m    124\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from google.colab import files\n","uploaded = files.upload()"]},{"cell_type":"markdown","metadata":{"id":"DWAlnuetfE9n"},"source":["**Module-6: This Modeule Is Final Modeule Use For Marking Attendance. In This Modeule We Will Load Coordinates File(DataSet.npz) And Embeddings File(Embeddings.npz). We Also Load Croped Images Obtained from Module-1 One By One And Compare Its Embeddings With Already Saved Embeddings Using SVM Classifier And Give Output As Label Of Matched Embedding.This Label Will Be the Name Of Student.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vsZRzWQd1W2t","outputId":"c5e941b3-541f-45ba-fa1e-002b68669b37"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Img_2.png', 'Img_3.png', 'Img_4.png', 'Img_5.png', 'Im_0.png', 'Im_1.png', 'Im_3.png', 'Im_4.png', 'Im_5.png', 'Im_6.png', 'Im_8.png', 'Im_9.png', 'Im_10.png', 'I_2.png', 'I_6.png']\n","Harshad\n","Bhikho\n","Harsh\n","Kashyap\n","Sidharth\n","Bhavya\n","Om\n","Manish\n","Tarik\n","HarshBhalala\n","Vatsal\n","Fenil\n","Smith\n","Unknown Face\n","Unknown Face\n"]}],"source":["from matplotlib import pyplot as plt\n","from PIL import Image\n","from numpy import asarray\n","from numpy import array\n","from mtcnn.mtcnn import MTCNN\n","from keras.models import load_model\n","from numpy import expand_dims\n","from numpy import reshape\n","from numpy import load\n","from numpy import max\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import Normalizer\n","from sklearn.svm import SVC\n","import os\n","\n","#extracting embeddings\n","def extract_embeddings(model,face_pixels):\n","  face_pixels = face_pixels.astype('float32')\n","  mean = face_pixels.mean()\n","  std  = face_pixels.std()\n","  face_pixels = (face_pixels - mean)/std\n","  samples = expand_dims(face_pixels,axis=0)\n","  yhat = model.predict(samples)\n","  return yhat[0]\n","directory='/content/drive/MyDrive/FaceNet/cropped/'\n","print(listdir('/content/drive/MyDrive/FaceNet/cropped/'))\n","dirs = os.listdir( directory )\n","model = load_model('/content/drive/MyDrive/FaceNet/facenet_keras.h5',compile=False)\n","data1 = load('/content/drive/MyDrive/FaceNet/DataSet.npz')\n","train_x,train_y = data1['arr_0'],data1['arr_1']\n","\n","data = load('/content/drive/MyDrive/FaceNet/Embeddings.npz')\n","trainx,trainy= data['arr_0'],data['arr_1']\n","\n","# i=0\n","name=[[]]\n","# data rows of csv file\n","\n","\n","for filename in dirs:\n","    Img = directory + filename\n","\n","#load data and reshape the image\n","    img1 = Image.open(Img)            \n","    img1 = img1.convert('RGB')  \n","    img1 = img1.resize((160,160))        \n","    pixels = asarray(img1)    \n","    testx = pixels.reshape(-1,160,160,3)\n","    # print(\"Input test data shape: \",testx.shape)\n","\n","    #find embeddings\n","    new_testx = list()\n","    for test_pixels in testx:\n","      embeddings = extract_embeddings(model,test_pixels)\n","      new_testx.append(embeddings)\n","\n","    new_testx = asarray(new_testx)  \n","    # print(\"Input test embedding shape: \",new_testx.shape)\n","    # print(trainy.shape[0])\n","    # print(\"Loaded data: Train=%d , Test=%d\"%(trainx.shape[0],new_testx.shape[0]))\n","\n","    #normalize the input data\n","    in_encode = Normalizer(norm='l2')\n","    trainx = in_encode.transform(trainx)\n","    new_testx = in_encode.transform(new_testx)\n","\n","    #create a label vector\n","    new_testy = trainy \n","    out_encode = LabelEncoder()\n","    out_encode.fit(trainy)\n","    trainy = out_encode.transform(trainy)\n","    new_testy = out_encode.transform(new_testy)\n","    \n","    #define svm classifier model \n","    model1 =SVC(kernel='linear', probability=True)\n","    model1.fit(trainx,trainy)\n","\n","    #predict\n","    predict_train = model1.predict(trainx)\n","    predict_test = model1.predict(new_testx)\n","\n","\n","    #get the confidence score\n","    probability = model1.predict_proba(new_testx)\n","    confidence = max(probability)\n","    # print(confidence)\n","\n","    if confidence > 0.45:\n","\n","      acc_train = accuracy_score(trainy,predict_train)\n","\n","      #display\n","      trainy_list = list(trainy)\n","      p=int(predict_test)\n","      if p in trainy_list:\n","        val = trainy_list.index(p)\n","        trainy = out_encode.inverse_transform(trainy)\n","        string = strftime('%H:%M:%S: %p')\n","        name.append([trainy[val],string])\n","        # print(trainy[val])\n","        # i+=1\n","      print(trainy[val])\n","    else:\n","      print(\"Unknown Face\")\n","if len(name) > 0:\n"," header = ['Name','Time']\n"," with open('Attendance', 'w') as f:\n","    writer = csv.writer(f)\n","    # write the header\n","    writer.writerow(header)\n","    for w in name:\n","      writer. writerow(w)\n","    "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29029,"status":"ok","timestamp":1653910495776,"user":{"displayName":"Dhameliya Arpit","userId":"14178346834218186043"},"user_tz":-330},"id":"WrZFAvdUF5r2","outputId":"ec45600f-e8c6-4d54-eebc-c283987e56a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["mv /content/drive/MyDrive/Facenet /content/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Rft55NLRwG1k","executionInfo":{"status":"ok","timestamp":1653910572908,"user_tz":-330,"elapsed":1020,"user":{"displayName":"Dhameliya Arpit","userId":"14178346834218186043"}},"outputId":"9fbc6adf-93d7-4967-b0f2-25592704bd54"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"Ml-1l8YylKDX"},"source":["**Module-7: This Module Will Be Used To Create Data Set.It Will Capture 100 Images Of Each Person Automatically And Store It At Specified Location Which Than Furthure Be Used For Training.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bh7MXIOBlBct"},"outputs":[],"source":["from google.colab.patches import cv2_imshow\n","import cv2\n","face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades+\"haarcascade_frontalface_default.xml\")\n","def face_cropped(img):\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n","    for(x, y, w, h) in faces:\n","        face_cropped = img[y:y+h, x:x+w]\n","        return face_cropped\n","\n","\n","\n","img_id = 0\n","cap=cv2.VideoCapture(0)\n","while True:\n","    ret,img=cap.read()\n","    cv2_imshow(img)\n","\n","    if face_cropped(img) is not None:\n","        img_id += 1\n","        face = cv2.resize(face_cropped(img), (450, 450))\n","        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n","        dir=\"/content/drive/MyDrive/FaceNet/train/kashyap/\"\n","        file_name_path=dir + str(img_id) + \".jpg\"\n","        print(file_name_path)   \n","        cv2.imwrite(file_name_path, face)\n","        cv2.putText(face, str(img_id), (50, 50),cv2.FONT_HERSHEY_COMPLEX, 2, (0, 255, 0), 2)\n","        cv2.imshow(\"Cropped Face\", face)\n","\n","        if cv2.waitKey(1) == 13 or int(img_id) == 100:  \n","            break\n","cv2.destroyAllWindows()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"facenet.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}